\input{slides/header.tex}

\title{Multimodal Generative Learning on the MIMIC-CXR Database}
\subtitle{A presentation of my semester project}
\author{Hendrik Klug}
\institute{Institute for Electrical Engineering, ETH}
\begin{document}
	\begin{frame}
		\titlepage
	\end{frame}
	\section{Introduction}

	    \begin{frame}
	        In this work, we applied a method for self-supervised, multimodal and generative training from \cite{thomas_multimodal} on the MIMIC-CXR Database \cite{johnson2019mimic}.
	    \end{frame}

		\subsection{Multimodal, Unsupervised, Generative models}
			\begin{frame}{The General Idea}
				\py{pytex_printonly(script='scripts/generalidea_graph.py', data = '')}
			\end{frame}

        \begin{frame}{Multimodal, Unsupervised Generative Learning On Medical Data}
            \begin{itemize}
                \item No need for labeled data
                \item Can extract features from multiple modalities
                \item Can generate \textit{coherent} samples from one input modality
            \end{itemize}
        \end{frame}

    \section{Background}
        \subsection{The MoPoE-VAE}
            \begin{frame}{The Mixture-of-Products-of-Experts-VAE}
            Combination of:
                \begin{itemize}
                    \item The Product-of-Experts (PoE) from \cite{wu2018multimodal}
                    \item The Mixture-of-Experts (MoE) from \cite{shi2019variational}
                \end{itemize}
                \vspace{\baselineskip}
                Both differ in their choice of the joint posterior approximation functions.
            \end{frame}

            \begin{frame}{The PoE-VAE}

                \begin{itemize}
                    \item Uses a geometric mean: the joint posterior is a product of individual posteriors
                    %p(z|x_1,...,x_N)) \propto p(z) \prod _{i=1} ^N \tilde{q}(z|x_i)
                \begin{equation}
                    q_{\Phi}(z|x_{1:M})=\prod _m q_{\Phi_m}(z|x_m)
                \end{equation}
                    \item Results in a good approximation of the joint distribution but struggles in optimizing the individual experts.
                \end{itemize}

            \end{frame}

            \begin{frame}{The MoE-VAE}
            \begin{itemize}
                \item Uses an arithmetic mean
                \begin{equation}
                    q_{\Phi}(z|x_{1:M})=\sum _m \alpha_m\cdot q_{\Phi_m}(z|x_m)
                \end{equation}
                \item Optimizes individual experts well but is not able to learn a distribution that is sharper than any of its experts.
            \end{itemize}

            \end{frame}

            \begin{frame}{The Mixture-of-Products-of-Experts-VAE}
                The generalized multimodal ELBO utilizes the PoE to get the posterior approximation of a subset $\xsubset \in \mathcal{P}(\mathbb{X})$:

				\begin{equation}
					\tilde{q}_{\phi}(\textbf{z}|\xsubset)=PoE(\{q_{\phi_j}(\textbf{z}|\textbf{x}_j) \forall \textbf{x}_j \in \xsubset\}) \propto \prod _{\textbf{x}_j \in \xsubset}q_{\phi_j}(\textbf{z}|\textbf{x}_j)
				\end{equation}
				And the MoE to get the joint posterior:
				\begin{equation}
					q_{\phi}(\textbf{z}|\mathbb{X}) = \frac{1}{2^3} \sum _{\textbf{x}_k \in \mathbb{X}} \tilde{q}_{\phi} (\textbf{z}|\mathbb{X}_k)
				\end{equation}
            \end{frame}

            \begin{frame}{Frame Title}

			\py{pytex_printonly(script='scripts/mopoe_graph.py', data = '')}

            \end{frame}

    \section{Methods}
        \subsubsection{The MIMIC-CXR Database}

            \begin{frame}
                \begin{enumerate}
                    \item Implemented word encoding (show dictionary example)
                    \item tested image size
                    \item tested beta
                    \item tested class dim
                \end{enumerate}
            \end{frame}

            \begin{frame}
                \py{pytex_fig('scripts/rand_sample_from_dataset.py', conf='slides/dataset.conf', label='dataset_samples', caption='Samples from the dataset.')}
            \end{frame}

            \begin{frame}{The word encoding}

            \end{frame}
\printbibliography

\end{document}
