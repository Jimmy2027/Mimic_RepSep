\section{Methods}

\subsection{Generalized multimodal ELBO}

We assume that the MIMIC-CXR database contains $N$ i.i.d. samples $\{\xseti\}^N_{i=1}$, each of which is a set of 3 modalities $\mathbb{X}^{(i)} = \{\textbf{x}_j^{(i)}\}^3_{j=1}$.

Assuming the data was generated by some random process described by a joint hidden random variable $z$, the parameters of the VAE model are updated with the objective of maximising the marginal log likelihood:
\begin{equation}
    \log p_{\theta}(\{\mathbb{X}^{(i)}\}_{i=1}^3) = \sum _{i=1}^3 \log p_{\theta}(\mathbb{X}^{(i)})
\end{equation}
Which can be rewritten as:
\begin{equation}
    \label{log_likelihood_objective}
    \log p_{\theta}(\mathbb{X}^{(i)}) = D_{KL}(q_{\phi}(\textbf{z}|\xset^{(i)})||p_{\theta}(\textbf{z}|\xseti)) + \mathcal{L}(\theta, \phi; \xseti)
\end{equation}
With $\mathcal{L}(\theta, \phi; \xseti)$ being the evidence lower bound (ELBO) on the marginal log-likelihood of the i-th set:
\begin{equation}
    \mathcal{L}(\theta, \phi; \xseti) := \mathbb{E}_{\approxdistri}[\log p_{\theta}(\xseti|\textbf{z})] - D_{KL}(\approxdistri||p_{\theta}(\textbf{z}))
\end{equation}

Here, the posterior distribution is assumed to be Gaussian distributed $\approxdistri \sim \mathcal{N}(\textbf{z}; \bm{\mu}, \bm{\sigma}^2\textbf{I}_n)$, with $n$ the number of latent space dimensions.
The Gaussian distribution is the most popular choice as prior found in the literature.\\
While minimising the KL-divergence between the approximate $\approxdistri$ and the true posterior distribution $\truedistr$ is infeasible, the ELBO forms a tractable objective to approximate the joint data distribution $\log p_{\theta}(\xseti)$.
Since $p_{\theta}(\xseti) \geq \elbo$, maximising the ELBO, minimises the KL-divergence between the approximate and the true posterior distribution in \cref{log_likelihood_objective}.
If the approximate posterior distribution equals the true posterior distribution, the marginal log likelihood is equal to the ELBO.\\
The objective can thus be rewritten to:
\begin{equation}
    \arg \min _{\phi} D_{KL}(\approxdistri||p_{\theta}(\textbf{z}|\xseti))
\end{equation}

For 3 modalities, there are $2^3$ different subsets contained in the powerset $\mathcal{P}(\mathbb{X})$.
The generalized multimodal ELBO utilizes the PoE to get the posterior approximation of a subset $\xsubset$:

\begin{equation}
    \tilde{q}_{\phi}(\textbf{z}|\xsubset)=PoE(\{q_{\phi_j}(\textbf{z}|\textbf{x}_j) \forall \textbf{x}_j \in \xsubset\}) \propto \prod _{\textbf{x}_j \in \xsubset}q_{\phi_j}(\textbf{z}|\textbf{x}_j)
\end{equation}
And the MoE to get the joint posterior:
\begin{equation}
    q_{\phi}(\textbf{z}|\mathbb{X}) = \frac{1}{2^3} \sum _{\textbf{x}_k \in \mathbb{X}} \tilde{q}_{\phi} (\textbf{z}|\mathbb{X}_k)
\end{equation}
The objective $\lmopoe$ for learning a joint distribution of the different data types $\mathbb{X}$ can then be written as:

\begin{equation}
    \lmopoe := \mathbb{E}_{\approxdistr}[\log p_{\theta}(\xset|\textbf{z})] - D_{KL} \left( \frac{1}{2^3} \sum _{\textbf{x}_k \in \powerset} \tilde{q}_{\phi} (\textbf{z}|\mathbb{X}_k)||p_{\theta}(\textbf{z})\right)
\end{equation}

\cite{thomas_gener-ELBO} shows that $\lmopoe$ minimizes the convex combination of KL-divergences of the powerset $\powerset$.
Assuming conditional independence between the likelihoods $p_{\theta}(\xset _m|\textbf{z} _m)$ as well as between the posterior approximations of the subspaces $\tilde{q}_{\phi}(\textbf{z}|\mathbb{X}_k)$, the explicit loss function with respect to which the parameters of the VAE are updated is then:

\begin{equation}
    \label{explicit_loss}
    \textbf{Loss} = \sum _m ^{3} -\omega _w \cdot \log p_{\theta}(\xset_m|\textbf{z}_m) + \beta \cdot \frac{1}{2^3} \sum _{\textbf{x}_k \in \powerset} D_{KL} \left(\tilde{q}_{\phi}(\textbf{z}|\mathbb{X}_k)||p_{\theta}(\textbf{z})\right)
\end{equation}

The parameters $\omega _w$ and $\beta$ are chosen empirically.

\subsection{MIMIC-CXR Database}
The MIMIC-CXR Database \cite{johnson2019mimic} is a large publicly available dataset of chest radiographs with free-text radiology reports containing 377,110 images corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center in Boston, MA.
In this work, three modalities were extracted from the database: frontal and lateral chest radiographs together with their corresponding text reports.
Only datapoints where all three modalities are present were selected.
Every sample is labeled with one or more of the following categories: 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices'.
For the evaluation of our method, we created a label "Finding", indicating if the sample is labeled with any of the previously listed labels.
We note that the dataset is highly unbalanced, namely there are \py{boilerplate.print_finding_counts()} samples that are annotated with the "Finding" label in the training set and \py{boilerplate.print_nofinding_counts()} samples that are not.

\subsection{The Model}
The MoPoE-VAE has an encoder and decoder for each modality, in this work both are the same for the image modalities (frontal and lateral scans).
A full representation of the different parts of the complete model can be found in the supplementary material \cref{fig:flowchart}.

Both the text encoder and decoder are made out of 6 blocks following the ResNet \cite{he2016deep} architecture, each consisting of 2 series of batch normalization, a relu activation function, a 1d convolution and a dropout layer.
At the end of each block, the output is weighted with the input.
%An on Imagenet \cite{imagenet_cvpr09} pretrained densenet \cite{huang2018densely} is used as the image encoder.
Similar to the text models, the image encoder and decoder are made of 5 blocks, each consisting of 2 series of batch normalization, a relu activation function, a 2d convolution and a dropout layer.
The encoder models use a convolution for the downsampling of the input to a bottleneck while the decoder models use a transposed convolution for upscaling.

Making use of the assumption that the posterior distributions are Gaussian, the mean $\mu$ and the variance $\sigma$ for each modality are extracted at the bottleneck by sending the output of the corresponding encoder model into two linear layers, one for the mean and one for the variance.

\subsubsection{Preprocessing}
For our purposes, all images were resized to \py{boilerplate.print_img_shape()}.
Every word that occurs at least \py{boilerplate.print_flag_attribute('word_min_occ')} times in all the text reports is mapped to an index.
Using this mapping each sentence is encoded into a sequence of indices.
All sentences with a word count above \py{boilerplate.print_flag_attribute('len_sequence')} are truncated and all sentences consisting of less words are padded with a padding token "$<pad>$" such that all text samples are of length \py{boilerplate.print_flag_attribute('len_sequence')}.

\subsection{Evaluation of the Latent Representations}
To evaluate if the MoPoE-VAE is able to extract characteristic information and compress them as a latent representation in a meaningful manner, we evaluate the separability of the latent space via logistic regression, as implemented by the freely available software package scikit-learn \cite{scikit}.
If the classifier can separate the latent space into the corresponding classes, we conclude that the posterior approximations are meaningful.
For this a linear classifier was trained on 500 samples from the training set, encoded by the encoder parts of the MoPoE-VAE.
One classifier for each Class and for each latent space (so $3*2^3 = 24$ classifiers in total) is used.
The classifiers were then tested on the encoded test set.
In order to compare our method with a supervised classification method, we compare the classification performance with ResNet \cite{he2016deep} classifiers, trained in a supervised manner for each modality individually.

\subsection{Coherence evaluation}
To evaluate the coherence of the generated samples, they are classified using the same supervised ResNet classifiers described in the previous section.
If all modalities of a sample, generated and given as conditioner, are classified as having the same label, they are considered coherent.
In this manner, the coherence of the MoPoE-VAE is evaluated over the whole test set.

\subsection{Common Details}
% see common details section here: https://arxiv.org/pdf/1911.03393.pdf
For all the presented results of the MoPoE-VAE, the model was trained for 150 epochs.
We used the Adam optimizer \cite{adam} with a learning rate of $\py{boilerplate.print_flag_attribute('initial_learning_rate')}$.
The parameter $\beta$ from \cref{explicit_loss} was chosen empirically to be \py{boilerplate.print_flag_attribute('beta')} and the dimension of the latent representation for each class is set to \py{boilerplate.print_class_dim()}.
Both hyperparameters were chosen via a grid-search where the parameters of the best performing model in terms of separability of the latent representation and generation coherence were selected.
For all unimodal posterior approximations, we assume Gaussian distributions $\mathcal{N}(\textbf{z}; \bm{\mu}, \bm{\sigma}^2\textbf{I}_n)$ where $n$ is the number of latent space dimensions.
In all experiments, the mixture components are equally weighted with $\frac{1}{\#components}$.
We publish \cite{mimic_repsep} the code used to generate all figures and results in this document as an open source and reproducible RepSep \cite{ioanas2018reproducible} document.
This work is thus completely verifiable and expandable.

