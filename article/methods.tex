\section{Methods}
\subsection{Generalized multimodal ELBO}

We assume that the MIMIC-CXR database contains $N$ i.i.d. samples $\{\xseti\}^N_{i=1}$, each of which is a set of 3 modalities $\mathbb{X}^{(i)} = \{\textbf{x}_j^{(i)}\}^3_{j=1}$.

Assuming the data was generated by some random process described by a joint hidden random variable $z$, the parameters of the VAE model are updated with the objective of maximising the marginal log likelihood:
\begin{equation}
    \log p_{\theta}(\{\mathbb{X}^{(i)}\}_{i=1}^3) = \sum _{i=1}^3 \log p_{\theta}(\mathbb{X}^{(i)})
\end{equation}
Which can be rewritten as:
\begin{equation}\label{log_likelihood_objective}
    \log p_{\theta}(\mathbb{X}^{(i)}) = D_{KL}(q_{\phi}(\textbf{z}|\xset^{(i)})||p_{\theta}(\xseti)) + \mathcal{L}(\theta, \phi; \xseti)
\end{equation}
With $\mathcal{L}(\theta, \phi; \xseti)$ being the evidence lower bound (ELBO) on the marginal log-likelihood of the i-th set:
\begin{equation}
    \mathcal{L}(\theta, \phi; \xseti) := \mathbb{E}_{\approxdistri}[\log p_{\theta}(\xseti|\textbf{z})] - D_{KL}(\approxdistri||p_{\theta}(\textbf{z}))
\end{equation}
While minimising the KL-divergence between the approximate ($\approxdistri$) and the true posterior distribution ($\truedistr$) is infeasible, the ELBO forms a tractable objective to approximate the joint data distribution $\log p_{\theta}(\xseti)$.
Since $p_{\theta}(\xseti) \geq \elbo$, maximising the ELBO, minimises the KL-divergence between the approximate and the true posterior distribution in \cref{log_likelihood_objective}.
If the approximate posterior distribution equals the true posterior distribution, the marginal log likelihood is equal to the ELBO.\\
The objective can thus be rewritten to:
\begin{equation}
    \arg \min _{\phi} D_{KL}(\approxdistri||p_{\theta}(\textbf{z}|\xseti))
\end{equation}

For 3 modalities, the are $2^3$ different subsets contained in the powerset $\mathcal{P}(\mathbb{X})$. %todo is this clear?
The generalized multimodal ELBO utilizes the PoE to get the posterior approximation of a subset $\xsubset$:

\begin{equation}
    \tilde{q}_{\phi}(\textbf{z}|\xsubset)=PoE(\{q_{\phi_j}(\textbf{z}|\textbf{x}_j) \forall \textbf{x}_j \in \xsubset\}) \propto \prod _{\textbf{x}_j \in \xsubset}q_{\phi_j}(\textbf{z}|\textbf{x}_j)
\end{equation}
And the MoE to get the joint posterior:
\begin{equation}
    q_{\phi}(\textbf{z}|\mathbb{X}) = \frac{1}{2^3} \sum _{\textbf{x}_k \in \mathbb{X}} \tilde{q}_{\phi} (\textbf{z}|\mathbb{X}_k)
\end{equation}
The objective $\lmopoe$ for learning a joint distribution of the different data types $\mathbb{X}$ can then be written as:
\begin{equation}
    \lmopoe := \mathbb{E}_{\approxdistr}[\log p_{\theta}(\xset|\textbf{z})] - D_{KL} \left( \frac{1}{2^3} \sum _{\textbf{x}_k \in \powerset} \tilde{q}_{\phi} (\textbf{z}|\mathbb{X}_k)||p_{\theta}(\textbf{z})\right)
\end{equation}
\cite{thomas_gener-ELBO} shows that $\lmopoe$ minimizes the convex combination of KL-divergences of the powerset $\powerset$.
The explicit loss function with respect to which the parameters of the VAE are updated is then:
\begin{equation}
    \textbf{Loss} = \sum _m ^{\#mods} \omega _w \cdot \log p_{\theta}(\xset_m|\textbf{z}_m) + \beta \cdot \frac{1}{2^3} \sum _{\textbf{x}_k \in \mathbb{X}} \tilde{q}_{\phi}(\textbf{z}|\mathbb{X}_k)
\end{equation}

\subsection{MIMIC-CXR Database}
The MIMIC-CXR Database \cite{johnson2019mimic} is a large publicly available dataset of chest radiographs with free-text radiology reports containing 377,110 images corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center in Boston, MA.
In this work, three modaliteis were extracted from the database: frontal and lateral chest radiographs and their corresponding text reports. Only datapoints where all three modalities are present are selected.
%todo write about three classes (pleural effusion, lung opacity and support devices).
All images are resized to \imgsize.

A mapping is created between each word that occurs at least \minwordocc times in all the text reports and an index.
Using this mapping each sentence is encoded into a sequence of indices.
All sentences with a word count above \sentlen are truncated and all sentences consisting of less words are padded with a padding token "$<pad>$" such that all text samples are of length \sentlen.

\subsection{Common Details}
% see common details section here: https://arxiv.org/pdf/1911.03393.pdf
As optimizer, we use the Adam optimizer \cite{adam} with a learning rate of \learningrate.